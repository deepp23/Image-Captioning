{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "modules=list(model.children())[:-1]\n",
    "model=nn.Sequential(*modules).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess=transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "def get_img_embed(img_path):\n",
    "    img=Image.open(img_path).convert('RGB')\n",
    "    img_tensor=preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb=model(img_tensor)\n",
    "\n",
    "    return img_emb.squeeze(0).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='/Users/deepmalikpalthya/Downloads/Designer.jpeg'\n",
    "print(get_img_embed(img_path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions={}\n",
    "with open('captions.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            filename, caption = line.strip().split(',', 1)\n",
    "            if filename not in captions:\n",
    "                captions[filename]=[]\n",
    "            captions[filename].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for filename, caption_list in captions.items():\n",
    "    updated_captions = []  \n",
    "    for caption in caption_list:\n",
    "        words = caption.strip().split()\n",
    "        updated_captions.append(words)\n",
    "        vocabulary.update(words)\n",
    "    captions[filename] = updated_captions\n",
    "vocabulary.add('<start>')\n",
    "vocabulary.add('<end>')\n",
    "vocabulary.add('<pad>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "data_loader = []\n",
    "count = 0\n",
    "\n",
    "for filename, captions in captions.items():\n",
    "    if count >= 200:\n",
    "        break\n",
    "    img_embed = get_img_embed(f'Images/{filename}')\n",
    "\n",
    "    for caption in captions:\n",
    "        caption_idx = [word_to_idx[word] for word in ['<start>'] + caption + ['<end>']]\n",
    "        data_loader.append([img_embed, torch.tensor(caption_idx)])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions).unsqueeze(0)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, 1, self.lstm.hidden_size)\n",
    "        outputs, _ = self.lstm(embeddings, (features, c0))\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 256\n",
    "hidden_size = 2048\n",
    "vocab_size = len(vocabulary) \n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_layers=1\n",
    "grad_clip = 5.0 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(input_size, hidden_size, vocab_size, num_layers)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    batches = [data_loader[i:i+batch_size] for i in range(0, len(data_loader), batch_size)]\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    total_loss = 0\n",
    "    for batch in batches:\n",
    "        optimizer.zero_grad()\n",
    "        img_embeds = batch[0][0]\n",
    "        captions = torch.tensor(batch[0][1])  \n",
    "        pad_token = word_to_idx['<pad>']\n",
    "        max_length = 200\n",
    "        pad_length = max_length - captions.shape[-1]\n",
    "        padded_captions = F.pad(captions, (0, pad_length), \"constant\", pad_token)\n",
    "        outputs = decoder(img_embeds, padded_captions)\n",
    "        outputs = outputs[:, :-1, :]  \n",
    "        targets = padded_captions[1:]  \n",
    "        mask = (targets != pad_token)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss = loss * mask.view(-1)\n",
    "        loss = loss.sum() / mask.sum()  \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Average Loss: {average_loss:.4f}')\n",
    "\n",
    "\n",
    "torch.save(decoder.state_dict(), 'imgcap.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, max_length=50):\n",
    "    image_embedding = get_img_embed(image_path)\n",
    "    start_token = torch.tensor(word_to_idx['<start>']).unsqueeze(0)\n",
    "    caption = []\n",
    "    input_sequence = start_token\n",
    "    cn = torch.zeros(1, 1, 2048)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Call the forward function directly\n",
    "        input_sequence = decoder.embed(input_sequence).unsqueeze(0)\n",
    "        output, (hn, cn) = decoder.lstm(input_sequence, (image_embedding, cn))\n",
    "        word_idx = output[:, -1].argmax(dim=1).item()  # Get the predicted word index at the last time step\n",
    "\n",
    "        if word_idx == word_to_idx['<end>']:\n",
    "            break\n",
    "\n",
    "        if word_idx in idx_to_word:  # Check if the word index is in the vocabulary\n",
    "            word = idx_to_word[word_idx]\n",
    "            caption.append(word)\n",
    "            input_sequence = torch.tensor([word_idx])\n",
    "        else:\n",
    "            if word_idx < len(word_to_idx):  # Check if the word index is within the vocabulary size\n",
    "                word = 'unk'\n",
    "                caption.append('<unk>')\n",
    "                input_sequence = torch.tensor([word_to_idx['<unk>']])\n",
    "            else:\n",
    "                # Handle out-of-range word indices\n",
    "                word = 'unk'\n",
    "                caption.append('<unk>')\n",
    "                input_sequence = torch.tensor([word_to_idx['<unk>']])\n",
    "\n",
    "        image_embedding = hn\n",
    "        cn = cn\n",
    "        print(hn.shape)\n",
    "        print(cn.shape)\n",
    "\n",
    "    return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_caption('Designer.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(image_path, beam_width=3, max_length=50):\n",
    "    image_embedding = get_img_embed(image_path)\n",
    "    start_token = word_to_idx['<start>']\n",
    "    end_token = word_to_idx['<end>']\n",
    "    c0 = torch.zeros(1, 1, 2048)\n",
    "\n",
    "    beam = [([start_token], 0.0)]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            input_seq = torch.tensor(seq)\n",
    "            embeddings = decoder.embed(input_seq).unsqueeze(0)\n",
    "            output, _ = decoder.lstm(embeddings, (image_embedding, c0))\n",
    "            word_scores = output[:, -1].squeeze(0)\n",
    "            word_scores = F.log_softmax(word_scores, dim=-1)\n",
    "            top_scores, top_indices = word_scores.topk(beam_width, dim=-1)\n",
    "\n",
    "            for new_score, new_index in zip(top_scores, top_indices):\n",
    "                new_seq = seq + [new_index.item()] \n",
    "                new_score = score + new_score.item()\n",
    "                new_beam.append((new_seq, new_score))\n",
    "\n",
    "                if new_index.item() == end_token:\n",
    "                    break\n",
    "\n",
    "        new_beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        beam = new_beam\n",
    "\n",
    "        if beam[0][0][-1] == end_token:\n",
    "            break\n",
    "\n",
    "    best_seq = beam[0][0][1:-1]  # Remove start and end tokens\n",
    "    caption = ' '.join(idx_to_word[idx] for idx in best_seq)\n",
    "    return caption\n",
    "\n",
    "print(beam_search('Designer.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
